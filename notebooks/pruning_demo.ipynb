{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef91f2f",
   "metadata": {},
   "source": [
    "# Three Ways to Prune a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import autograd\n",
    "from autograd import numpy as np\n",
    "from autograd import elementwise_grad as egrad\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "fn_type = type(lambda x: x)\n",
    "nparray = type(np.random.rand(1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80873ba",
   "metadata": {},
   "source": [
    "## Defining the forward pass and sgd updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2227988",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = lambda x: x * (x > 0.0)\n",
    "\n",
    "def forward(x: nparray, layers: list) -> nparray:\n",
    "\n",
    "    for layer in layers[:-1]:\n",
    "        x = relu(x @ layer) \n",
    "\n",
    "    x = x @ layers[-1]\n",
    "\n",
    "    return x\n",
    "\n",
    "relu = lambda x: x * (x > 0.0)\n",
    "\n",
    "def forward(x: nparray, layers: list) -> nparray:\n",
    "\n",
    "    for layer in layers[:-1]:\n",
    "        x = relu(x @ layer) \n",
    "\n",
    "    x = x @ layers[-1]\n",
    "\n",
    "    return x\n",
    "\n",
    "def initialize_layer_weights(in_dim: int, out_dim: int) -> nparray:\n",
    "\n",
    "    sigma = np.sqrt(2 / (in_dim + out_dim))\n",
    "\n",
    "    return sigma * np.random.randn(in_dim, out_dim) \n",
    "\n",
    "def initialize_weights(dimensions: list) -> list:\n",
    "\n",
    "    layers = []\n",
    "    for dims in dimensions:\n",
    "\n",
    "        layers.append(initialize_layer_weights(dims[0], dims[1]))\n",
    "\n",
    "    return layers\n",
    "\n",
    "def initialize_model(my_seed: int, in_dim: int,\\\n",
    "        h_dim: int, out_dim: int, number_hidden: int) -> list:\n",
    "\n",
    "    dims = [[in_dim, h_dim]]\n",
    "    for layer_index in range(number_hidden):\n",
    "        dims.append([h_dim, h_dim])\n",
    "\n",
    "    dims.append([h_dim, out_dim])\n",
    "\n",
    "    np.random.seed(my_seed)\n",
    "\n",
    "    return initialize_weights(dims)\n",
    "\n",
    "nll_loss_fn = lambda y, pred: -np.mean(y*np.log(sm(pred)) + (1-y)*np.log(1. - sm(pred)))\n",
    "\n",
    "\n",
    "def compute_loss(x: nparray,\\\n",
    "        y: nparray,\\\n",
    "        loss_function: fn_type,\\\n",
    "        layers: list) -> np.float64:\n",
    "\n",
    "\n",
    "    predicted = forward(x, layers)\n",
    "\n",
    "    loss = loss_function(y, predicted)\n",
    "\n",
    "    return loss\n",
    "\n",
    "grad_loss = autograd.grad(compute_loss, argnum=3)  \n",
    "\n",
    "def sgd_update(layers: list, grad_layers: list, lr: float) -> list:\n",
    "\n",
    "    new_layers = []\n",
    "    for index, grad_layer in enumerate(grad_layers):\n",
    "\n",
    "        # multiplying by abs value of layer \n",
    "        # freezes weights with value zero \n",
    "        update = - lr * grad_layer * (np.abs(layers[index]) > 0.0)\n",
    "        new_layers.append(layers[index] + update)\n",
    "\n",
    "    return new_layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5bf572",
   "metadata": {},
   "source": [
    "# Sanity Check: Fitting random vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c31dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(128,64)\n",
    "targets = np.random.rand(128,10)\n",
    "mse_loss_fn = lambda y, p: np.mean((y-p)**2)\n",
    "lr = 1e-3\n",
    "\n",
    "dims = [[64,32], [32,32], [32,10]]\n",
    "\n",
    "layers = initialize_weights(dims)\n",
    "\n",
    "for step in range(1000):\n",
    "\n",
    "    grad_layers = grad_loss(x, targets, mse_loss_fn, layers)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        loss = compute_loss(x, targets, mse_loss_fn, layers) \n",
    "        print(f\"loss at step {step} = {loss:.3f}\")\n",
    "\n",
    "    layers = sgd_update(layers, grad_layers, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a293ba",
   "metadata": {},
   "source": [
    "## Pruning nodes by gradient magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba951ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_skeletonize(x: nparray, nodes: list, layers: list):\n",
    "    # special forward pass with nodes for dE/da\n",
    "\n",
    "    for layer, alpha in zip(layers[:-1], nodes):\n",
    "        \n",
    "        x = relu((x * alpha) @ layer)\n",
    "\n",
    "    x = (x * nodes[-1]) @ layers[-1]\n",
    "\n",
    "    return x\n",
    "   \n",
    "def compute_skeleton_loss(x: nparray,\\\n",
    "        y: nparray,\\\n",
    "        loss_function: fn_type,\\\n",
    "        nodes: list,\\\n",
    "        layers: list) -> np.float64:\n",
    "\n",
    "    predicted = forward_skeletonize(x, nodes, layers)\n",
    "\n",
    "    loss = loss_function(y, predicted)\n",
    "\n",
    "    return loss\n",
    "\n",
    "grad_skeleton_loss = autograd.grad(compute_skeleton_loss, argnum=3) \n",
    "\n",
    "def prune_node(layers: list,\\\n",
    "        grad_nodes: list) -> list:\n",
    "    # prunes only the least important input node \n",
    "\n",
    "    new_layers = layers\n",
    "\n",
    "    prune_indices = []\n",
    "    lowest_grad = float(\"Inf\") \n",
    "    for index, grad_layer in enumerate(grad_nodes[1:]):\n",
    "\n",
    "        prune_indices.append(np.argmin(grad_layer))\n",
    "\n",
    "        if grad_layer[prune_indices[-1]] < lowest_grad:\n",
    "            lowest_grad = grad_layer[prune_indices[-1]]\n",
    "            prune_index = index\n",
    "\n",
    "    p = prune_indices[prune_index]\n",
    "    new_layer = layers[prune_index+1][0:p, :]\n",
    "    new_layer = np.append(new_layer,\\\n",
    "            layers[prune_index+1][p+1:, :], axis=0) \n",
    "    new_layers[prune_index+1] = new_layer\n",
    "\n",
    "    new_layer_1 = layers[prune_index][:, 0:p]\n",
    "    new_layer_1 = np.append(new_layer_1,\\\n",
    "            layers[prune_index][:, p+1:], axis=1)\n",
    "    new_layers[prune_index] = new_layer_1\n",
    "\n",
    "    return new_layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c428d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pruning weights by their second derivative of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import elementwise_grad as egrad\n",
    "\n",
    "grad2_loss = egrad(egrad(compute_loss, argnum=3), argnum=3)\n",
    "\n",
    "def prune_weights_by_grad2(layers: list,\\\n",
    "        grad2_layers: list,\\\n",
    "        prune_per_layer: int=10,\\\n",
    "        initial_threshold: float=1e-5) -> list:\n",
    "\n",
    "    new_layers = []\n",
    "\n",
    "    for layer, grad2_layer in zip(layers, grad2_layers):\n",
    "        threshold = 1.0 * initial_threshold\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "        \n",
    "            prunable_weights = np.sum(\\\n",
    "                    np.abs(grad2_layer) < threshold)\n",
    "\n",
    "            if prunable_weights >= prune_per_layer:\n",
    "                done = True\n",
    "            else:\n",
    "                threshold *= 2.\n",
    "\n",
    "        new_layer = layer * (np.abs(grad2_layer) > threshold)\n",
    "        new_layers.append(new_layer)\n",
    "\n",
    "    return new_layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9baf61",
   "metadata": {},
   "source": [
    "## Weight magnitude pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_weights_by_magnitude(layers: list,\\\n",
    "        prune_per_layer: int=10,\\\n",
    "        initial_threshold: float=1e-3) -> list:\n",
    "\n",
    "    return prune_weights_by_grad2(layers,\\\n",
    "            layers, prune_per_layer,\\\n",
    "            initial_threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d18e3b4",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eca522",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = lambda x: np.exp(x)/(np.sum(np.exp(x), \\\n",
    "        axis=-1, keepdims=True))\n",
    "nll_loss_fn = lambda y, pred: -np.mean(\\\n",
    "        y*np.log(sm(pred)) + (1-y)*np.log(1. - sm(pred)))\n",
    "\n",
    "def indices_to_one_hot(y: nparray, max_index=None) -> nparray: \n",
    "\n",
    "    if max_index is None:\n",
    "        max_index = np.max(y)\n",
    "\n",
    "    y_target = np.zeros((y.shape[0], max_index+1))\n",
    "\n",
    "    for ii in range(y.shape[0]):\n",
    "        y_target[ii, y[ii]] = 1.0\n",
    "\n",
    "    return y_target\n",
    "\n",
    "def compute_accuracy(y: nparray, predicted: nparray) -> float:\n",
    "\n",
    "    return np.mean(y.argmax(-1) == predicted.argmax(-1))\n",
    "\n",
    "def prune_mode_0(layers: list,\\\n",
    "        train_x: nparray=None, train_y: nparray=None) -> list:\n",
    "    return layers\n",
    "\n",
    "def prune_mode_1(layers: list,  train_x: nparray, train_y: nparray) -> list:\n",
    "\n",
    "    nodes = [np.ones(elem.shape[0]) for elem in layers]\n",
    "    grad_nodes = grad_skeleton_loss(\\\n",
    "            train_x, train_y, nll_loss_fn, nodes, layers)\n",
    "\n",
    "    layers = prune_node(layers, grad_nodes)\n",
    "\n",
    "    return layers\n",
    "\n",
    "def prune_mode_2(layers: list, train_x: nparray, train_y: nparray) -> list: \n",
    "\n",
    "    grad2_layers = grad2_loss(\\\n",
    "            train_x, train_y, nll_loss_fn, layers)\n",
    "    layers = prune_weights_by_grad2(layers, grad2_layers)\n",
    "\n",
    "    return layers \n",
    "\n",
    "def prune_mode_3(layers: list,\\\n",
    "        train_x: nparray=None, train_y: nparray=None) -> list:\n",
    "\n",
    "    return prune_weights_by_magnitude(layers)\n",
    "\n",
    "def retrieve_prune_fn(mode: int=0):\n",
    "\n",
    "    if mode == 0:\n",
    "        return prune_mode_0\n",
    "    elif mode ==1:\n",
    "        return prune_mode_1\n",
    "    elif mode ==2:\n",
    "        return prune_mode_2\n",
    "    elif mode ==3:\n",
    "        return prune_mode_3\n",
    "def split_digits(my_seed: int=13) -> tuple:\n",
    "\n",
    "    x, y_indices = sklearn.datasets.load_digits(return_X_y = True)\n",
    "    x = x / np.max(x)\n",
    "    y = indices_to_one_hot(y_indices)\n",
    "\n",
    "    np.random.seed(my_seed)\n",
    "    np.random.shuffle(x)\n",
    "\n",
    "    np.random.seed(my_seed)\n",
    "    np.random.shuffle(y)\n",
    "\n",
    "    split_validation = int(0.1*x.shape[0])\n",
    "\n",
    "    val_x = x[:split_validation,:]\n",
    "    val_y = y[:split_validation,:]\n",
    "    train_x = x[split_validation:,:]\n",
    "    train_y = y[split_validation:,:]\n",
    "\n",
    "    return train_x, train_y, val_x, val_y\n",
    "\n",
    "def print_progress(layers: list=None,\\\n",
    "        batch_x: nparray=None,\\\n",
    "        batch_y: nparray=None,\\\n",
    "        tag: str=\"train\",\\\n",
    "        step: int=0,\\\n",
    "        verbose: bool=True):\n",
    "\n",
    "    if layers is None:\n",
    "        # print column labels\n",
    "        msg = f\"split, step, loss, accuracy\"\n",
    "    else:\n",
    "        loss = compute_loss(batch_x, batch_y, nll_loss_fn, layers) \n",
    "\n",
    "        msg = f\"{tag}, {step:05}, {loss:.4f}, \"\n",
    "\n",
    "        predicted = forward(batch_x, layers)\n",
    "        accuracy = compute_accuracy(batch_y, predicted)\n",
    "\n",
    "        msg += f\"{accuracy:.4f}\\n\"\n",
    "\n",
    "    if verbose:\n",
    "        print(msg)\n",
    "\n",
    "    return msg\n",
    "\n",
    "def train(my_seed: int=13,\\\n",
    "        number_epochs: int=100,\\\n",
    "        mode: int=0,\\\n",
    "        lr: float=1e-3,\\\n",
    "        verbose: bool=True):\n",
    "    \"\"\"\n",
    "    mode 0 - no pruning\n",
    "    mode 1 - pruning nodes (Mozer and Smolensky 1989)\n",
    "    mode 2 - pruning w by 2nd derivative (LeCun et al. 1990)\n",
    "    mode 3 - by magnitude (e.g. Han et al. 2015 and others)\n",
    "    \"\"\"\n",
    "\n",
    "    my_prune_fn = retrieve_prune_fn(mode)\n",
    "    batch_size = 1024\n",
    "    number_prunes = 28\n",
    "    h_dim = 16\n",
    "    number_hidden = 2\n",
    "    display_every = number_epochs // 10\n",
    "\n",
    "    train_x, train_y, val_x, val_y = split_digits(my_seed)\n",
    "\n",
    "    in_dim = train_x.shape[-1]\n",
    "    out_dim = train_y.shape[-1]\n",
    "\n",
    "    layers = initialize_model(my_seed, in_dim, h_dim, out_dim, number_hidden)\n",
    "\n",
    "    ticket_layers = copy.deepcopy(layers)\n",
    "\n",
    "    progress = print_progress()\n",
    "    for step in range(number_epochs):\n",
    "\n",
    "        if step % display_every == 0:\n",
    "            progress += print_progress(layers,\\\n",
    "                    train_x, train_y, tag=\"train\", verbose=verbose, step=step)\n",
    "            progress += print_progress(layers,\\\n",
    "                    val_x, val_y, tag=\"valid\", verbose=verbose, step=step)\n",
    "\n",
    "        batch_indices = np.random.randint(train_x.shape[0],\\\n",
    "                size=(batch_size,))\n",
    "        batch_x, batch_y = train_x[batch_indices], train_y[batch_indices]\n",
    "        grad_layers = grad_loss(batch_x, batch_y, nll_loss_fn, layers)\n",
    "\n",
    "        layers = sgd_update(layers, grad_layers, lr=lr)\n",
    "\n",
    "    for pruning_step in range(number_prunes):\n",
    "        layers = my_prune_fn(layers,  train_x, train_y)\n",
    "        ticket_layers = my_prune_fn(ticket_layers, train_x, train_y)\n",
    "\n",
    "    progress += print_progress(layers,\\\n",
    "            train_x, train_y, tag=\"train_post_prune\", verbose=verbose, step=step)\n",
    "    progress += print_progress(layers,\\\n",
    "            val_x, val_y, tag=\"valid_post_prune\", verbose=verbose, step=step)\n",
    "\n",
    "    for steps in range(display_every):\n",
    "        \n",
    "        grad_layers = grad_loss(train_x, train_y, nll_loss_fn, layers)\n",
    "        layers = sgd_update(layers, grad_layers, lr=lr)\n",
    "\n",
    "    progress += print_progress(layers,\\\n",
    "            train_x, train_y, tag=\"train_retrained\", verbose=verbose, step=step)\n",
    "    progress += print_progress(layers,\\\n",
    "            val_x, val_y, tag=\"valid_retrained\", verbose=verbose, step=step)\n",
    "\n",
    "    save_dir = os.path.join(\"parameters\", f\"mode_{mode}\")\n",
    "\n",
    "    if os.path.exists(save_dir):\n",
    "        pass\n",
    "    else:\n",
    "        os.system(f\"mkdir -p {save_dir}\")\n",
    "\n",
    "    print(f\"model shape with mode {mode} pruning\")\n",
    "\n",
    "    for ii, layer in enumerate(layers):\n",
    "        print(layer.shape, np.sum(np.abs(layer) > 0))\n",
    "        save_filepath = os.path.join(save_dir,f\"layer{ii}.npy\")\n",
    "        np.save(save_filepath, layer)\n",
    "\n",
    "    # examine lottery ticket hypothesis\n",
    "    pruned_dims = [layer.shape for layer in ticket_layers]\n",
    "    noticket_layers = initialize_weights(pruned_dims)\n",
    "\n",
    "    for step in range(number_epochs):\n",
    "        \n",
    "        batch_indices = np.random.randint(train_x.shape[0],\\\n",
    "                size=(batch_size,))\n",
    "        batch_x, batch_y = train_x[batch_indices], train_y[batch_indices]\n",
    "        ticket_grad_layers = grad_loss(batch_x, batch_y, nll_loss_fn, ticket_layers)\n",
    "        noticket_grad_layers = grad_loss(batch_x, batch_y, nll_loss_fn, noticket_layers)\n",
    "\n",
    "        ticket_layers = sgd_update(ticket_layers, ticket_grad_layers, lr=lr)\n",
    "        noticket_layers = sgd_update(noticket_layers, noticket_grad_layers, lr=lr)\n",
    "\n",
    "        if step % display_every == 0 or step == (number_epochs-1):\n",
    "            progress += print_progress(ticket_layers,\\\n",
    "                    train_x, train_y, tag=\"train_ticket\", verbose=verbose, step=step)\n",
    "            progress +=  print_progress(ticket_layers,\\\n",
    "                    val_x, val_y, tag=\"valid_ticket\", verbose=verbose, step=step)\n",
    "            progress +=  print_progress(noticket_layers,\\\n",
    "                    train_x, train_y, tag=\"train_noticket\", verbose=verbose, step=step)\n",
    "            progress +=  print_progress(noticket_layers,\\\n",
    "                    val_x, val_y, tag=\"valid_noticket\", verbose=verbose, step=step)\n",
    "\n",
    "    progress_filepath = f\"mode_{mode}_log.csv\"\n",
    "    with open(progress_filepath, \"w\") as f:\n",
    "        f.write(progress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665bff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 10000\n",
    "lr = 1e-1\n",
    "\n",
    "for mode in range(4):\n",
    "    train(number_epochs=number_of_epochs,mode=mode, lr=lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b33e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
